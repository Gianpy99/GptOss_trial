# Come Usare il Modello Fine-tuned con LM Studio

## ğŸ¯ LM Studio: La Soluzione PIÃ™ FACILE (Raccomandato!)

LM Studio puÃ² caricare direttamente i modelli HuggingFace senza conversione GGUF!

### Passaggi:

#### 1. **Crea il Modello Merged**
```bash
# Attiva environment
.\.venv_training\Scripts\Activate.ps1

# Esegui merge
python merge_adapter.py
```

Questo crea: `fine_tuned_models/f1_expert_merged/` (~8 GB)

#### 2. **Apri LM Studio**
- Scarica da: https://lmstudio.ai/
- Installa e apri

#### 3. **Importa il Modello**
- `File` â†’ `Import` â†’ `Import from local folder`
- Seleziona: `C:\Development\Ollama_wrapper\fine_tuned_models\f1_expert_merged`
- LM Studio lo carica automaticamente!

#### 4. **Usa il Modello**
- Vai su `Chat`
- Seleziona `f1_expert_merged` dalla lista
- Prova: "Tell me about Lewis Hamilton's performance in F1"

---

## ğŸ“Š Confronto Metodi

| Metodo | DifficoltÃ  | Tempo | Pro | Contro |
|--------|-----------|-------|-----|--------|
| **LM Studio** | ğŸŸ¢ Facile | 10 min | Supporta safetensors, UI grafica | Richiede installazione |
| **Ollama** | ğŸ”´ Difficile | 30-60 min | Integrato CLI | Serve conversione GGUF |
| **Python diretto** | ğŸŸ¡ Medio | 5 min | GiÃ  funziona | Nessuna UI |

---

## ğŸ”§ Alternative per Ollama

Se vuoi **davvero** usare Ollama, ecco le opzioni:

### Opzione A: Conversione GGUF con llama-cpp-python
```bash
# In .venv_training
pip install llama-cpp-python

# Merge
python merge_adapter.py

# Converti (ATTENZIONE: puÃ² non funzionare su Windows)
python -m llama_cpp.convert "./fine_tuned_models/f1_expert_merged" \
       --outfile "./fine_tuned_models/f1_expert.gguf" \
       --outtype f16
```

### Opzione B: Usa HuggingFace Spaces
1. Merge: `python merge_adapter.py`
2. Upload su HuggingFace (se hai account)
3. Usa: https://huggingface.co/spaces/ggml-org/gguf-my-repo
4. Scarica il GGUF generato
5. Importa in Ollama con Modelfile

### Opzione C: Wrapper Python + Ollama per Interfaccia
Usa il tuo `wrapper.py` esistente ma carica il modello merged:

```python
# Modifica wrapper.py per usare il merged model
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(
    "./fine_tuned_models/f1_expert_merged"
)
# Poi usa l'interfaccia di wrapper.py
```

---

## ğŸ“ Dove Sono i File

```
ğŸ“‚ C:\Development\Ollama_wrapper\
   ğŸ“‚ finetuning_projects\
      ğŸ“‚ f1_expert_fixed\
         ğŸ“‚ adapter\                    â† Adapter LoRA (solo differenze)
            ğŸ“„ adapter_model.safetensors  (45.5 MB)
            ğŸ“„ adapter_config.json
            
   ğŸ“‚ fine_tuned_models\               â† Dopo merge_adapter.py
      ğŸ“‚ f1_expert_merged\             â† Modello COMPLETO per LM Studio
         ğŸ“„ model.safetensors          (~8 GB)
         ğŸ“„ config.json
         ğŸ“„ tokenizer files...
         
      ğŸ“„ f1_expert.gguf                â† Dopo conversione (per Ollama)
         (~4-6 GB, quantizzato)
```

---

## ğŸ¯ RACCOMANDAZIONE FINALE

**Per te:**
1. âœ… **Esegui `merge_adapter.py`** (crea modello completo)
2. âœ… **Scarica LM Studio** (gratis, facile)
3. âœ… **Importa `f1_expert_merged`** in LM Studio
4. âœ… **Testa e confronta** con Ollama `gemma3:4b`

**Vantaggi LM Studio:**
- âœ… Carica safetensors direttamente (no GGUF necessario)
- âœ… UI grafica comoda
- âœ… Supporta quantizzazione automatica (4-bit, 8-bit)
- âœ… Server locale compatibile con API OpenAI
- âœ… Funziona su Windows senza problemi

**Se davvero vuoi Ollama:**
- Serve build llama.cpp su Windows (complesso)
- O usa servizio online per conversione GGUF
- O aspetta che Ollama supporti PEFT nativamente

Vuoi che esegua `merge_adapter.py` per te?
