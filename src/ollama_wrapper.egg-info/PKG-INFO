Metadata-Version: 2.4
Name: ollama-wrapper
Version: 0.0.0
Summary: Lightweight Python wrapper for a local Ollama HTTP API
Author: Gianpy99
License: MIT
Requires-Python: >=3.8
Description-Content-Type: text/markdown

# Ollama_wrapper

Wrapper Python leggero e completo per API HTTP Ollama locale (testato con modelli Gemma3).

Questo repository fornisce un'utilit√† in singolo file, `wrapper.py`, che offre una API di alto livello per:

- Chat bloccante e streaming contro un server Ollama locale
- Allegati multimodali (immagini, PDF) codificati come base64
- Salvataggio/caricamento sessioni (JSON sotto `ollama_sessions/`)
- Memoria (SQLite) per cronologia conversazioni e fatti a lungo termine (`ollama_memory.db`)
- Helper di gestione modelli di base e wrapper CLI sicuro per il binario `ollama`

Il progetto √® intenzionalmente piccolo e pensato per sperimentazione locale con un server Ollama (URL base predefinito: `http://localhost:11434/api`).

## üìã Indice
- [Requisiti](#requisiti)
- [Installazione](#installazione)
- [Utilizzo Rapido](#utilizzo-rapido)
- [Esempi Completi](#esempi-completi)
- [API Principale](#api-principale)
- [Configurazione](#configurazione)
- [Test](#test)
- [Contribuire](#contribuire)

## üîß Requisiti

- Python 3.8+
- Libreria `requests`
- Server Ollama in esecuzione (con modello `gemma3:4b` o simile)

## üöÄ Installazione

### Installazione rapida
```powershell
# Clona il repository
git clone https://github.com/Gianpy99/Ollama_wrapper.git
cd Ollama_wrapper

# Installa le dipendenze
pip install -r requirements.txt

# Installa il package in modalit√† editable
pip install -e .
```

### Verifica installazione
```powershell
# Esegui il test rapido
python test_wrapper.py

# Esegui la demo completa
python demo.py
```

## ‚ö° Utilizzo Rapido

### Chat semplice
```python
from ollama_wrapper import OllamaWrapper

# Crea il wrapper (usa gemma3:4b come default)
wrapper = OllamaWrapper()

# Chat semplice
response = wrapper.chat("Spiegami la ricorsione in termini semplici")
print(response['assistant'])
```

### Chat streaming
```python
# Streaming chat (i chunk arrivano man mano)
for chunk in wrapper.stream_chat("Scrivi una breve poesia su Marte"):
    print(chunk, end="", flush=True)
```

### Assistenti preconfigurati
```python
from ollama_wrapper import create_coding_assistant, create_creative_assistant

# Assistente per programmazione (temperatura bassa, prompt specifico)
coding = create_coding_assistant()
code_response = coding.chat("Scrivi una funzione Python per il quicksort")

# Assistente creativo (temperatura alta)
creative = create_creative_assistant()
story = creative.chat("Inventa una storia breve sui robot")
```

### Memoria e sessioni
```python
# Memorizza informazioni
wrapper.store_memory("linguaggio_preferito", "Python", "preferenze")

# Recupera informazioni
memory = wrapper.recall_memory("linguaggio_preferito")
print(memory)  # ('linguaggio_preferito', 'Python', 'preferenze')

# Salva/carica sessioni
wrapper.save_session("mia_sessione")
wrapper.load_session("mia_sessione")
```

## üìö Esempi Completi

### Allegati multimodali
```python
# Chat con allegati (immagini, PDF)
response = wrapper.chat(
    "Analizza questa immagine",
    files=["./immagine.jpg", "./documento.pdf"]
)
```

### Configurazione avanzata
```python
from ollama_wrapper import OllamaWrapper, ModelParameters

# Parametri personalizzati
params = ModelParameters(
    temperature=0.1,        # Creativit√† bassa per risposte precise
    top_p=0.9,
    max_tokens=2048,
    seed=42                 # Per riproducibilit√†
)

wrapper = OllamaWrapper(
    model_name="gemma3:4b",
    session_id="sessione_lavoro",
    parameters=params
)

# Imposta un prompt di sistema
wrapper.set_system_prompt("Sei un esperto consulente Python. Rispondi sempre con esempi di codice.")
```

### REPL interattivo
```python
from ollama_wrapper import interactive_repl

wrapper = OllamaWrapper()
interactive_repl(wrapper)  # Avvia REPL interattivo
```

### Gestione modelli
```python
# Lista modelli disponibili
models = wrapper.list_models()
print(models)

# Scarica nuovo modello
result = wrapper.pull_model("gemma3:12b")

# Informazioni su un modello
info = wrapper.show_model_info("gemma3:4b")
```

## üîß API Principale

### Classe OllamaWrapper

#### Costruttore
```python
OllamaWrapper(
    base_url="http://localhost:11434/api",
    model_name="gemma3:4b",
    session_id="default",
    parameters=None,
    memory_db_path="ollama_memory.db",
    prefer_cli=False
)
```

#### Metodi principali
- `chat(message, include_history=True, store_conversation=True, files=None, timeout=60)` - Chat bloccante
- `stream_chat(message, include_history=True, timeout=120)` - Chat streaming
- `set_system_prompt(prompt)` - Imposta prompt di sistema
- `save_session(name)` / `load_session(name)` - Gestione sessioni
- `store_memory(key, value, category="general")` - Memorizza fatto
- `recall_memory(key)` - Recupera fatto
- `search_memories(query, limit=20)` - Cerca nella memoria

### Assistenti preconfigurati
- `create_coding_assistant(session_id="coding")` - Per programmazione
- `create_creative_assistant(session_id="creative")` - Per contenuti creativi

### Classe MemoryManager
- `store_message(session_id, message)` - Memorizza messaggio conversazione
- `get_conversation_history(session_id, limit=100)` - Recupera cronologia
- `store_fact(key, value, category)` - Memorizza fatto
- `search_facts(query, limit=20)` - Cerca fatti

## ‚öôÔ∏è Configurazione

### URL base e modello predefinito
```python
DEFAULT_BASE_URL = "http://localhost:11434/api"
DEFAULT_MODEL = "gemma3:4b"
```

### Directory di output
- `ollama_sessions/` - File di sessione (JSON)
- `ollama_memory.db` - Database SQLite per memoria

### Parametri del modello
```python
ModelParameters(
    temperature=0.7,      # Creativit√† (0.0-1.0)
    top_p=0.9,           # Nucleus sampling
    top_k=None,          # Top-k sampling
    max_tokens=1024,     # Lunghezza massima risposta
    repeat_penalty=None, # Penalit√† ripetizione
    seed=None,           # Seed per riproducibilit√†
    num_ctx=None         # Contesto (lunghezza)
)
```

## üß™ Test

### Test rapido
```powershell
python test_wrapper.py
```

### Test completi
```powershell
python test_complete.py
```

### Demo interattiva
```powershell
python demo.py
```

### Test unitari
```powershell
python -m pytest tests/
```

## ü§ñ Fine-tuning e personalizzazione

Il wrapper √® progettato per essere facilmente estendibile per progetti di fine-tuning:

### Preparazione dati per fine-tuning
```python
# Esporta conversazioni per training
wrapper = OllamaWrapper(session_id="training_data")
history = wrapper.memory.get_conversation_history("training_data", limit=1000)

# Formatta per fine-tuning
training_data = []
for msg in history:
    training_data.append({
        "role": msg.role,
        "content": msg.content,
        "timestamp": msg.timestamp
    })
```

### Integrazione con altri servizi
```python
# Esempio: integrazione con logging avanzato
import logging

class ExtendedWrapper(OllamaWrapper):
    def chat(self, message, **kwargs):
        logging.info(f"Chat request: {message[:50]}...")
        response = super().chat(message, **kwargs)
        logging.info(f"Chat response: {response.get('status')}")
        return response
```

## üõ†Ô∏è Sviluppo e Contributi

### Note per sviluppatori
- Evita chiamate di rete reali nei test unitari; mocka `requests.get` e `requests.post`
- Test unitari utili da aggiungere:
  - Creazione schema `MemoryManager` e operazioni CRUD (usa file DB temporaneo)
  - `_build_messages()` con e senza `system_prompt` e con cronologia memorizzata
  - Gestione `stream_chat()` di stream JSON-line e chunk di testo (mocka `requests.post(..., stream=True)`)

### Casi limite e gotchas
- Gli endpoint streaming possono emettere linee non-JSON ‚Äî `stream_chat` fallback su chunk raw
- `pull_model()` aspetta linee di eventi streaming da `/pull` e ritorna eventi parsed (se `stream=True`) o body completo
- Le connessioni SQLite sono create con `check_same_thread=False` e ogni operazione apre una nuova connessione

### Struttura del progetto
```
Ollama_wrapper/
‚îú‚îÄ‚îÄ src/ollama_wrapper/          # Package principale
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py              # Export pubblici
‚îÇ   ‚îî‚îÄ‚îÄ wrapper.py               # Implementazione principale
‚îú‚îÄ‚îÄ examples/                    # Esempi di utilizzo
‚îú‚îÄ‚îÄ tests/                       # Test unitari e integrazione
‚îú‚îÄ‚îÄ scripts/                     # Script di utilit√†
‚îú‚îÄ‚îÄ demo.py                      # Demo interattiva
‚îú‚îÄ‚îÄ test_wrapper.py             # Test rapido
‚îú‚îÄ‚îÄ test_complete.py            # Test completi
‚îú‚îÄ‚îÄ requirements.txt            # Dipendenze
‚îú‚îÄ‚îÄ pyproject.toml             # Configurazione package
‚îî‚îÄ‚îÄ README.md                  # Questa documentazione
```

### Contribuire
- Mantieni le modifiche localizzate su `wrapper.py` quando possibile
- Aggiungi test sotto la directory `tests/`
- Se aggiungi nuove dipendenze, includile in `requirements.txt`

## üêõ Risoluzione problemi

### Problemi comuni

**Errore: "Connection refused"**
```bash
# Verifica che Ollama sia in esecuzione
curl http://localhost:11434/api/tags

# Avvia Ollama se non √® attivo
ollama serve
```

**Errore: "Model not found"**
```bash
# Lista modelli disponibili
ollama list

# Scarica il modello se necessario
ollama pull gemma3:4b
```

**Timeout su chat lunghe**
```python
# Aumenta il timeout
response = wrapper.chat("Domanda complessa", timeout=120)
```

**Problemi con memoria SQLite**
```python
# Resetta il database se corrotto
import os
os.remove("ollama_memory.db")
wrapper = OllamaWrapper()  # Ricreer√† il DB
```

## üìÑ Licenza

MIT License - vedi il file LICENSE per dettagli.

## ü§ù Supporto

- Apri un issue per bug o richieste di funzionalit√†
- Controlla la documentazione in `.github/copilot-instructions.md` per dettagli implementativi
- Esegui `python demo.py` per vedere esempi di utilizzo

---

**üöÄ Buon coding con Ollama!**
