# üéâ Fine-Tuning Integration - Summary

## What's Been Added

Your Ollama_wrapper project now has **full fine-tuning capabilities** integrated with Hugging Face and PEFT!

## üìÅ New Files

### Core Module
- **`src/ollama_wrapper/finetuning.py`** - Main fine-tuning module with FineTuningManager class

### Documentation
- **`FINETUNING_GUIDE.md`** - Comprehensive guide with examples, best practices, and troubleshooting
- **`requirements-finetuning.txt`** - Optional dependencies for fine-tuning

### Examples
- **`examples/example_finetuning_integration.py`** - Complete workflow with interactive menu
- **`examples/quick_start_finetuning.py`** - 5-minute quick start example

### Configuration
- Updated **`pyproject.toml`** - Added optional `[finetuning]` dependencies
- Updated **`src/ollama_wrapper/__init__.py`** - Exports FineTuningManager
- Updated **`requirements.txt`** - Added comments about optional dependencies
- Updated **`README.md`** - Added fine-tuning section

## üöÄ Quick Start

### 1. Install Dependencies
```powershell
# Base wrapper (required)
pip install -r requirements.txt

# Fine-tuning (optional)
pip install -r requirements-finetuning.txt
```

### 2. Run Quick Example
```powershell
python examples/quick_start_finetuning.py
```

### 3. Try Full Workflow
```powershell
# Interactive menu
python examples/example_finetuning_integration.py

# Or run specific steps
python examples/example_finetuning_integration.py step1
python examples/example_finetuning_integration.py step2
python examples/example_finetuning_integration.py step3
```

## üîë Key Features

### 1. **Seamless Integration**
```python
from ollama_wrapper import OllamaWrapper, FineTuningManager

# Use existing OllamaWrapper conversations for training
wrapper = OllamaWrapper()
wrapper.chat("What is Python?")

# Fine-tune using those conversations
manager = FineTuningManager()
dataset = manager.load_training_data_from_memory()
```

### 2. **Memory Efficient (QLoRA)**
```python
# Train 7B model with only ~6GB GPU RAM
manager = FineTuningManager(use_4bit=True)
```

### 3. **Small Adapters**
```python
# Adapters are only 2-50 MB vs 3-13 GB for full models
manager.save_adapter("my_adapter")  # Just a few megabytes!
```

### 4. **Multiple Data Sources**
```python
# From Ollama memory database
dataset = manager.load_training_data_from_memory()

# From JSON file
dataset = manager.load_training_data_from_json("training.json")
```

### 5. **Easy to Use**
```python
# Three simple steps
manager.load_model()
manager.setup_lora()
manager.train(dataset)
```

## üìö Usage Examples

### Basic Fine-Tuning
```python
from ollama_wrapper import FineTuningManager

manager = FineTuningManager(model_name="microsoft/phi-2")
manager.load_model()
manager.setup_lora(r=16, lora_alpha=32)

dataset = manager.load_training_data_from_memory()
tokenized = manager.tokenize_dataset(dataset)

manager.train(tokenized, num_epochs=3)
manager.save_adapter("my_adapter")
```

### Using Fine-Tuned Model
```python
from ollama_wrapper import create_finetuned_assistant

assistant = create_finetuned_assistant(
    "./fine_tuned_models/my_adapter",
    temperature=0.7
)

response = assistant.generate_text("User: Hello\n\nAssistant:")
```

### Hybrid Workflow
```python
from ollama_wrapper import OllamaWrapper, create_finetuned_assistant

# General queries -> Fast Ollama
ollama = OllamaWrapper()

# Specialized queries -> Fine-tuned model
specialist = create_finetuned_assistant("./my_adapter")

def smart_chat(query):
    if "python" in query.lower():
        return specialist.generate_text(f"User: {query}\n\nAssistant:")
    else:
        return ollama.chat(query)
```

## üéØ Use Cases

### 1. **Domain Specialist**
Train models on domain-specific conversations (medical, legal, technical)

### 2. **Style Adaptation**
Teach models your preferred writing style and response format

### 3. **Task-Specific Assistants**
Create specialized assistants for coding, writing, analysis, etc.

### 4. **Continual Learning**
Regularly update models with new conversations and feedback

### 5. **Multi-Expert System**
Create multiple specialized adapters and route queries intelligently

## üìñ Documentation

- **Quick Start**: `examples/quick_start_finetuning.py`
- **Full Guide**: `FINETUNING_GUIDE.md`
- **Complete Workflow**: `examples/example_finetuning_integration.py`
- **API Reference**: Docstrings in `src/ollama_wrapper/finetuning.py`

## üîß Configuration Options

### LoRA Parameters
```python
manager.setup_lora(
    r=16,                    # Rank (8-64, higher=more capacity)
    lora_alpha=32,          # Alpha (typically 2*r)
    lora_dropout=0.05,      # Dropout (0.0-0.1)
    target_modules=[...],   # Which layers to adapt
)
```

### Training Parameters
```python
manager.train(
    train_dataset=data,
    num_epochs=3,           # Training epochs
    batch_size=4,           # Samples per batch
    learning_rate=2e-4,     # Learning rate
    gradient_accumulation_steps=4,
)
```

### Generation Parameters
```python
assistant.generate_text(
    prompt="...",
    max_new_tokens=256,     # Max output length
    temperature=0.7,        # Randomness (0=deterministic)
    top_p=0.9,             # Nucleus sampling
    top_k=50,              # Top-k sampling
)
```

## üêõ Troubleshooting

### Out of Memory
```python
# Use 4-bit quantization
manager = FineTuningManager(use_4bit=True)

# Reduce batch size
manager.train(batch_size=1, gradient_accumulation_steps=16)
```

### Dependencies Not Found
```powershell
pip install -r requirements-finetuning.txt
```

### No Training Data
```python
# Create conversations first
wrapper = OllamaWrapper(session_id="training")
wrapper.chat("question 1")
wrapper.chat("question 2")
# ... etc
```

## üí° Best Practices

1. **Start Small**: Use smaller models (phi-2) for testing
2. **Quality > Quantity**: 50 good examples > 500 bad ones
3. **Use Evaluation Sets**: Always split data for validation
4. **Monitor Training**: Check loss curves to prevent overfitting
5. **Experiment**: Try different LoRA ranks and learning rates
6. **Save Checkpoints**: Use `save_steps` to avoid losing progress
7. **Consistent Formatting**: Use same prompt format in training and inference

## üéì Learning Resources

- **PEFT Docs**: https://huggingface.co/docs/peft
- **LoRA Paper**: https://arxiv.org/abs/2106.09685
- **QLoRA Paper**: https://arxiv.org/abs/2305.14314
- **Transformers**: https://huggingface.co/docs/transformers

## ü§ù Next Steps

1. Install dependencies: `pip install -r requirements-finetuning.txt`
2. Try quick start: `python examples/quick_start_finetuning.py`
3. Read full guide: Open `FINETUNING_GUIDE.md`
4. Create your first adapter with your own data!

## ‚ùì Questions?

- Check `FINETUNING_GUIDE.md` for detailed documentation
- Run examples in `examples/` directory
- Read docstrings in `src/ollama_wrapper/finetuning.py`

---

**Happy Fine-Tuning! üöÄ**
