rank: 16
learning_rate: 0.001
batch_size: 32
num_epochs: 10
weight_decay: 0.01
lora_alpha: 32
lora_dropout: 0.1
lora_init: "normal"
lora_layers: [0, 1, 2]  # Specify which layers to apply LoRA to
use_fp16: true
logging_steps: 50
save_steps: 500
output_dir: "./output"